Трансформер для машинного перевода
Реализация модели трансформера для задач машинного перевода с поддержкой обучения на ограниченных ресурсах.




Проект включает следующие ключевые компоненты:

- Токенизатор

    BPE (Byte Pair Encoding) токенайзер - реализован с нуля
    
    Поддержка обучения токенизатора на корпусе текста
    
    Кодирование/декодирование текста в субтокены

- Модель трансформера

    Энкодер - собственный реализованный класс
    
    Декодер - собственный реализованный класс
    
    Остальные компоненты (нормализация, механизм внимания и т.д.) взяты из torch.nn



Особенности обучения

    Учитывая ограничения вычислительных ресурсов (Google Colab), реализованы следующие механизмы:



Мониторинг обучения

    Интеграция с Neptune.ai для логирования лоссов и метрик в реальном времени
    
    Визуализация процесса обучения через дашборд Neptune

![Лоссы](https://github.com/vAdimusprog/optimus_prime/raw/main/images/loss.jpg)

![Метрика](https://github.com/vAdimusprog/optimus_prime/raw/main/images/BLUE.jpg)


Сохранение прогресса

    Чекпоинты - регулярное сохранение состояния модели во время обучения
    
    Возможность возобновления обучения с последнего чекпоинта
    
    Сохранение лучших весов модели
